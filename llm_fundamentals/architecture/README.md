# üèóÔ∏è Transformer Architecture

Build a GPT-style transformer model from scratch.

## Files

| File | Description |
|------|-------------|
| `01_transformer_architecture.py` | Complete GPT model |
| `02_transformer_helpers.py` | Helper functions |

## What You'll Learn

- Embedding layers
- Position embeddings
- Attention mechanisms
- Transformer blocks
- Layer normalization
- Forward pass

## Run It

```bash
python 01_transformer_architecture.py
```

## Model Configuration

The model uses GPT-2 style configuration:
- Vocabulary: 200,256 tokens
- Context: 1,024 tokens
- Embedding: 768 dimensions
- Attention heads: 12
- Layers: 12

## Next Steps

‚Üí Move to `pre_training/` to train the model!
